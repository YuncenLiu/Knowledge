## 优化 - 精确一次性消费

至少一次消费：保证数据不丢失，可能存在多次消费问题

最多一次消费：主要保证不会重复，可能存在数据丢失问题

精确一次消费：消息只会被消费一次！



#### 丢失数据

kafka 在崩溃之前调整了偏移量 offset，即使消费者并没有消费到数据，重启后， kafka 依旧会在新的偏移量开始，次时就丢失数据



#### 重复消费

消费者优先消费数据，后提交 offset ，此时消费者已经成功的消费到了数据，在提交 offset 时失败， kafka 则认为消费者没有消费数据，下次消费依旧在偏移量之前消费。



> 目前：Kafka 默认 5秒 做一次提交偏移量
>
> `enable.auto.commit`  默认值是 `true`， 采用自动提交机制
>
> `auto.commit.interval.ms` 默认值时 `5000` ，单位毫米



### 解决方案

#### 1、利用事务

利用 Zookeeper、Redis 这种强一致性工具将读出数据和提交偏移量放到一个事务中。

好处：是可以精准的保证一次性消费；

问题是：

1. 数据必须放在关系型数据库中。
2. 事务本身性能低下
3. 如果是分布式情况，在解决分布式事务上，会难得多。

==**所以不考虑**==



#### 2、后置提交 offset + 幂等方案

首先保证数据不丢失，先考虑 `至少一次消费` 情况，然后清除重复数据。



### 手动提交偏移量流程

Kafka 在 0.9 版本之后，偏移量保存在 `__consumer_offsets` 中





## 优化 - 消息发送问题

#### 缓冲区问题

Kafka 消息发送分为 同步发送 和 异步发送，默认使用 异步发送，Kafka 在发送消息时，会先将消息发送到缓冲区，待缓冲区写满或者到达指定时间后，才会真正将缓冲区数据写入 Broker。

假设消息在缓冲区，还未到 Broker 时，Kafka 发生故障，而此时 offset 已经提交出去了，则产生缓冲区问题。



### 解决方案

#### 1、同步发送

将消息修改为同步发送，保证每条数据都能发送到 Broker，但是性能十分低下



#### 2、提交 offset 手动 flush 到 broker

Kafka 提供了 flush 对象，强制刷新缓冲区到 Broker 中



在 Scala 代码中，将 foreach 修改为 foreachPartition

```scala
 rdd.foreachPartition(
   jsonObjIter => {
     for (jsonObj <- jsonObjIter) {
       // ...
     }
   }
 )
```

