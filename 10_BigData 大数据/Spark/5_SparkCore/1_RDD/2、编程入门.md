#### 执行入口 SparkContext 对象

```python
# coding:utf8
from pyspark import SparkConf, SparkContext

if __name__ == '__main__':
    # .setMaster("local[*]")
    conf = SparkConf().setAppName("WordContHelloWorld")

    # 通过 SparkConf 对象构建 SparkContext 对象
    sc = SparkContext(conf=conf)
```

本质上只有构建出 SparkContext，基于它才能执行后续的 API 调用和计算

#### RDD 创建

##### 1、通过并行化集合创建

```python
rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9])
```

##### 2、读取对象文件

```python
rdd = sc.textFile("hdfs://hadoop01:9820/input/1.txt")
```





### RDD 算子

分布式集合对象上的 API 称之为 `算子`

1. Transformation 转化算子

	返回值仍旧是 RDD称之为 转化算子

	特性：这类算子 lazy 懒加载的，如果没有 action 算子， Transformation 算子是不工作的

	```python
	words_rdd = file_rdd.flatMap(lambda line: line.split(" "))
	```

2. Action 算子

	转化成 非 RDD（最终得出结果）

	```python
	result_add.collect()
	```

> 对于这两类算子，Transformation 相当于构建计划，action 是一个指令，只有当 action 指令执行后，Transformation 才会真正工作、流水式的处理数据



#### 常用 Transformation 算子

**Map 算子 语法**

```properties
rdd.map(func)
```

![image-20231016132507718](images/2%E3%80%81%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/image-20231016132507718.png)

**flatMap 算子**：对 rdd 执行 map 操作，然后进行 `解除嵌套` 操作

```properties
rdd.flatMap(func)
```



![image-20231016133847777](images/2%E3%80%81%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/image-20231016133847777.png)

**reduceByKey 算子**：针对` KV 型`RDD，自动按照 key 分组，然后根据你提供的聚合逻辑，完成`组内数据value `的聚合操作

```properties
rdd.reduceByKey(func)
```

==注意：== reduceByKey 中接收的函数，只负责聚合，不理会分组

```python
rdd = sc.parallelize([('a', 1), ('a', 1), ('b', 1), ('a', 1), ('b', 1)])
# reduceByKey 对于相同 key 的数据执行相加
print(rdd.reduceByKey(lambda a, b: a + b).collect())
# [('b', 2), ('a', 3)]
```

**groupBy 算子**：将 rdd 数据进行分组

```python
rdd = sc.parallelize([('a', 1), ('a', 1), ('a', 1), ('b', 1), ('b', 1)])
result = rdd.groupBy(lambda t: t[0])
print(result.map(lambda t: (t[0], list(t[1]))).collect())
# [('b', [('b', 1), ('b', 1)]), ('a', [('a', 1), ('a', 1), ('a', 1)])]
```

**Filter 算子**：过滤想要的数据进行保留

只保留基数

```python
rdd = sc.parallelize([1, 2, 3, 4, 5, 6])
# 只保留基数
result = rdd.filter(lambda x: x % 2 == 1)
# [1, 3, 5]
```

**distinct 算子：**对RDD数据进行去重

**union 算子：**2个rdd合并成1个rdd返回

**join 算子：**

```python
# join 关联，默认以 key 进行关联
join()
leftOuterJoin()
rightOuterJoin()
```

**intersection 算子：**2个 rdd 的交集，返回一个新 rdd

