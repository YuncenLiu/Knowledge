# 调度集群

[toc]

k8s 内 pod 由 scheduler 调度，分发到合适的 node 节点，scheduler 调度时会考虑 node 节点资源情况，port 使用情况，volume 使用情况等，在此基础上，我们也可以控制 pod 的调度，但是需要考虑很多问题

+ 公平，如何保证每个节点资源合理运用，不造成一个节点忙死，一个节点闲死
+ 资源高效利用，集群所有资源最大化被使用，内存、磁盘、CPU等因素
+ 效率，调度的性能要好，能够尽快对大批量 pod 完成调度工作
+ 灵活，运行用户这自定义需求控制调度逻辑



#### 固定节点

##### Pod.spec.nodeSelector

使用 label 标签的方式，通过

```sh
# 查看节点标签
kubectl get nodes --show-labels
# 打标签
kubectl label nodes k8s-node-01 mariadb=mariadb
# 删除 node-01 节点 mariabd 标签
kubectl label nodes k8s-node-01 mariadb-
```

![image-20241230104330288](images/18%E3%80%81%E8%B0%83%E5%BA%A6%E9%9B%86%E7%BE%A4/image-20241230104330288.png)



##### Pod.spec.nodeName

使用 `kubectl get nodes` 通过节点名称固定 pod

![image-20241230104609730](images/18%E3%80%81%E8%B0%83%E5%BA%A6%E9%9B%86%E7%BE%A4/image-20241230104609730.png)



### 集群调度原理

#### Scheduler 调度步骤

1. 首先用户通过 kubernetes 客户端 kubectl 提交创建 Pod 的 YAML 文件，向 Kubernetes 系统发起资源请求，该资源被要求提交到 Kubernetes 系统
2. Kubernetes 系统中，用户通过命令行工具 kubectl 向 kubernetes 集群即 APISever 发送 Post 请求，创建 Pod 请求。
3. APIServer 接收到请求后把创建 Pod 的信息存储到 Etcd 中，从集群运行那一刻，资源调度系统 Scheduler 就会去监控 APIServer
4. 通过 APIServer 得到创建 Pod 的信息，Scheduler 采用 watch 机制，一旦 Etcd 存储 Pod 信息成功便会立即通知 APIServer
5. APIServer 会立即把 Pod 创建的消息通知 Scheduler，Scheduler 发现 Pod 熟悉中的 DestNode 为空时候，便会立刻出发调度运行。
6. 而创建这个 Pod 对象，在调度中有三个阶段，阶段预选，节点优选，节点选定，从而筛选出最佳的节点
	1. 节点预选，基于一系列预选规则对每个节点进行检查，将那些不符合条件的节点过滤，从而完成节点预选
	2. 节点优选，对预选出的节点进行顺序排序，选出最合适的Pod运行对象节点
	3. 节点选定，从优先级排序结果中选出优先级最高的节点运行 Pod，此类节点大于1时，随机选择



#### 集群调度策略

k8s 将资源分为 两种属性

1. 可压缩资源 CPI循环，Disk I/O 带宽，都是可以被限制回收，对于一个 Pod 来说可以降低这些资源的使用了而不去杀 Pod
2. 不可压缩资源，内存，此片，一般不杀 Pod 就无法回收。



#### 节点亲和度调度

节点亲和性规则，required 硬亲和性，不能商量，必须执行、preferred 软亲和性，可以商量，选择执行

+ 硬亲和性规则不满足时，Pod 会置于 Pending 状态，软亲和性规则不满足时，会选择一个不匹配的节点
+ 当节点标签改变会不再符合此节点亲和性规则时，不会将 Pod 从改节点移出，仅对新建的 Pod 对象生效



### 硬亲和性

requiredDuringSchedulingIgnoredDuringExecution

+ 方式一：Pod使用 spec.nodeSelector (基于等值关系);Pod使用 spec.nodeName ==参考label==
+ 方式二：Pod使用 spec.affinity 支持 matchExpressions 属性 (复杂标签选择机制)



#### spec.affinity 

通过`kubectl get nodes --show-labels` 找到一个对应的 label 标签 `kubernetes.io/hostname=k8s-node-01`  

写出下面 pod 内容

![image-20241230113129605](images/18%E3%80%81%E8%B0%83%E5%BA%A6%E9%9B%86%E7%BE%A4/image-20241230113129605.png)

operator 运算规则

+ In：在某个列表中
+ NoIn：不在列表中
+ Gt：大于某个值
+ Lt：小于某个值
+ Exists：某个 label 存在
+ DoesNotExist：某个 label 不存在



当设置硬清和性设置的 label 匹配不上，就 Pending 状态，无法启动

![image-20241230113755866](images/18%E3%80%81%E8%B0%83%E5%BA%A6%E9%9B%86%E7%BE%A4/image-20241230113755866.png)



### 软亲和性

preferredDuringSchedulingIgnoredDuringExecution

+ 柔性控制逻辑，当条件不满足时，能接受被编排于其他不符合条件的节点之上
+ 权重 weight 定义优先级，1-100 值越大优先级越高



即使是 100 的权重，设置为错误的 label 标签，无法匹配，也不会 Pending

![image-20241230114322833](images/18%E3%80%81%E8%B0%83%E5%BA%A6%E9%9B%86%E7%BE%A4/image-20241230114322833.png)

![image-20241230114335642](images/18%E3%80%81%E8%B0%83%E5%BA%A6%E9%9B%86%E7%BE%A4/image-20241230114335642.png)



### 污点和容忍度

查看节点详情，查看 `Taints` 参数 `node-role.kubernetes.io/control-plane:NoSchedule` 这是集群默认设置的

```sh
kubectl describe nodes k8s-master-01

kubectl describe nodes k8s-node-01
```

effect 定于排斥集群

+ NoSchedule，不能容忍，但仅影响调度过程，已调度上去的pod不受影响，仅对新增加的pod生效。 
+ PreferNoSchedule，柔性约束，节点现存Pod不受影响，如果实在是没有符合的节点，也可以调度上来。 解释说明：表示 k8s 将不会将 Pod 调度到具有该污点的 Node 上 。
+ NoExecute，不能容忍，当污点变动时，Pod对象会被驱逐。 解释说明：表示 k8s 将不会将 Pod 调度到具有该污点的 Node 上，同时会将 Node 上已经存在的Pod 驱逐出去 



创建污点语法，直接把所有 node2 节点上运行的 pod 撵走

```sh
kubectl taint nodes k8s-node-02 node-role.kubernetes.io/control-plane:NoExecute

# 删除污点
kubectl taint nodes k8s-node-02 node-role.kubernetes.io/control-plane:NoExecute-
```

![image-20241230115711368](images/18%E3%80%81%E8%B0%83%E5%BA%A6%E9%9B%86%E7%BE%A4/image-20241230115711368.png)

AGE 为 8s 的都是从 node-02 上被撵走的pod资源。



#### 在 Pod 上定义容忍度时

1. 等值比较，容忍度与污点在 key、value、effect 三者完全匹配
2. 存在性判断 key、effect 完全匹配，value 使用空值

一个节点可以配置多个污点，一个 Pod 也可以有多个污点



设置一个污点 `offline=shutdown` 是自定义的名称

```sh
kubectl taint node k8s-node-02 offline=shutdown:NoSchedule
```

![image-20241230131238768](images/18%E3%80%81%E8%B0%83%E5%BA%A6%E9%9B%86%E7%BE%A4/image-20241230131238768.png)



